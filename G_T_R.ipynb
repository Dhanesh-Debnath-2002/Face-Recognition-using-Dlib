{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d6dae7-eaa3-4eac-b2bc-51f898cbad8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### Collect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b423e0fc-6bbe-459b-a2a8-abefb29d43df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter name of person: qwer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\n",
      "Dataset\\qwer\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'face_region_gray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17728\\927602450.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m# Apply the rotation to the cropped face region\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mface_aligned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarpAffine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_region_gray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdesiredFaceWidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'face_region_gray' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from imutils.face_utils import FaceAligner\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "shape_predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "face_aligner = FaceAligner(shape_predictor, desiredFaceWidth=200)\n",
    "\n",
    "#face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "name = input(\"Enter name of person:\")\n",
    "\n",
    "path = 'Dataset'\n",
    "print(path)\n",
    "directory = os.path.join(path, name)\n",
    "print(directory)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory, exist_ok = 'True')\n",
    "\n",
    "number_of_images = 0\n",
    "MAX_NUMBER_OF_IMAGES = 100\n",
    "count = 0\n",
    "\n",
    "while number_of_images < MAX_NUMBER_OF_IMAGES:\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #faces = face_cascade.detectMultiScale(frame, 1.3, 5)\n",
    "    faces = detector(frame_gray)\n",
    "    if len(faces) == 1:\n",
    "        face = faces[0]\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "        face_img = frame_gray[y-50:y + h+100, x-50:x + w+100]\n",
    "        # Calculate the angle between the eyes\n",
    "        dY = right_eye.y - left_eye.y\n",
    "        dX = right_eye.x - left_eye.x\n",
    "        angle = np.degrees(np.arctan2(dY, dX)) - 180\n",
    "\n",
    "# Calculate the scale for resizing\n",
    "        desiredFaceWidth = 200\n",
    "        scale = desiredFaceWidth / w\n",
    "\n",
    "# Calculate the rotation matrix\n",
    "        M = cv2.getRotationMatrix2D(eyes_center, angle, scale)\n",
    "\n",
    "# Apply the rotation to the cropped face region\n",
    "        face_aligned = cv2.warpAffine(face_region_gray, M, (desiredFaceWidth, h))\n",
    "\n",
    "        if count == 5:\n",
    "            cv2.imwrite(os.path.join(directory, str(name+str(number_of_images)+'.jpg')), face_aligned)\n",
    "            number_of_images += 1\n",
    "            count = 0\n",
    "        count += 1\n",
    "\n",
    "#         face_aligned = face_aligner.align(frame, frame_gray, face)\n",
    "\n",
    "#         if count == 5:\n",
    "#             cv2.imwrite(os.path.join(directory, str(name+str(number_of_images)+'.jpg')), face_aligned)\n",
    "#             number_of_images += 1\n",
    "#             count = 0\n",
    "#         #print(count)\n",
    "#         count+=1\n",
    "        \n",
    "        \n",
    "    cv2.imshow('Collecting the Face Data', frame)\n",
    "    \n",
    "\n",
    "    if(cv2.waitKey(1) & 0xFF == ord('q')):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Dataset completed!...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237a336-bd67-4b01-9ff6-be6a554caae3",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2057d3d-0022-477c-863e-f72bba2d96fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from sklearn import neighbors\n",
    "import face_recognition\n",
    "from face_recognition.face_recognition_cli import image_files_in_folder\n",
    "import pickle\n",
    "def train(train_dir, model_save_path, n_neighbors=2, knn_algo='ball_tree', verbose=False):\n",
    "    X = []\n",
    "    y = []\n",
    "    # Loop through each person in the training set\n",
    "    for class_dir in os.listdir(train_dir):\n",
    "        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n",
    "            continue\n",
    "        # Loop through each training image for the current person\n",
    "        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n",
    "            image = face_recognition.load_image_file(img_path)\n",
    "            face_bounding_boxes = face_recognition.face_locations(image)\n",
    "            #print( \"processing :\",img_path)\n",
    "            if len(face_bounding_boxes) != 1:\n",
    "                # If there are no people (or too many people) in a training image, skip the image.\n",
    "                if verbose:\n",
    "                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n",
    "            else:\n",
    "                # Add face encoding for current image to the training set\n",
    "                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n",
    "                y.append(class_dir)\n",
    "\n",
    "    # Determine how many neighbors to use for weighting in the KNN classifier\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int(round(math.sqrt(len(X))))\n",
    "        if verbose:\n",
    "            print(\"Chose n_neighbors automatically:\", n_neighbors)\n",
    "    # Create and train the KNN classifier\n",
    "    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n",
    "    knn_clf.fit(X, y)\n",
    "    # Save the trained KNN classifier\n",
    "    if model_save_path is not None:\n",
    "        with open(model_save_path, 'wb') as f:\n",
    "            pickle.dump(knn_clf, f)\n",
    "        print(\"Training completed!...\")\n",
    "    return knn_clf\n",
    "train(\"Dataset\",\"classifier/trained_knn_model.clf\") # add path here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244e1d7-df70-4f52-aaa5-2d5fc3104107",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa8938-faf4-41e6-90e9-fc62eaa06f09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2,time\n",
    "import pickle\n",
    "import face_recognition\n",
    "from face_recognition.face_recognition_cli import image_files_in_folder\n",
    "def predict(img_path, knn_clf=None, model_path=None, threshold=0.6): # 6 needs 40+ accuracy, 4 needs 60+ accuracy\n",
    "    if knn_clf is None and model_path is None:\n",
    "        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\n",
    "    # Load a trained KNN model (if one was passed in)\n",
    "    if knn_clf is None:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            knn_clf = pickle.load(f)\n",
    "    # Load image file and find face locations\n",
    "    img = img_path\n",
    "    face_box = face_recognition.face_locations(img)\n",
    "    # If no faces are found in the image, return an empty result.\n",
    "    if len(face_box) == 0:\n",
    "        return []\n",
    "    # Find encodings for faces in the test iamge\n",
    "    faces_encodings = face_recognition.face_encodings(img, known_face_locations=face_box)\n",
    "    # Use the KNN model to find the best matches for the test face\n",
    "    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=2)\n",
    "    matches = [closest_distances[0][i][0] <= threshold for i in range(len(face_box))]\n",
    "    accu=[100-closest_distances[0][i][0]*100 for i in range(len(face_box))]\n",
    "    # Predict classes and remove classifications that aren't within the threshold\n",
    "    return [(pred, loc,accu) if rec else (\"Unknown Face \", loc,accu) for pred, loc, rec,accu in zip(knn_clf.predict(faces_encodings),face_box,matches,accu)]\n",
    "webcam = cv2.VideoCapture(0) #  0 to use webcam \n",
    "while True:\n",
    "    # Loop until the camera is working\n",
    "    rval = False\n",
    "    while(not rval):\n",
    "        # Put the image from the webcam into 'frame'\n",
    "        (rval, frame) = webcam.read()\n",
    "        if(not rval):\n",
    "            print(\"Failed to open webcam. Trying again...\")\n",
    "\n",
    "\n",
    "    # start_frame=time.time()        \n",
    "    # Flip the image (optional)\n",
    "    frame=cv2.flip(frame,1) # 0 = horizontal ,1 = vertical , -1 = both\n",
    "    frame_copy = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    frame_copy=cv2.cvtColor(frame_copy, cv2.COLOR_BGR2RGB)\n",
    "    predictions = predict(frame_copy, model_path=\"classifier/trained_knn_model.clf\")\n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    for name, (top, right, bottom, left),accuracy in predictions:\n",
    "        top *= 4 #scale back the frame since it was scaled to 1/4 in size\n",
    "        right *= 4\n",
    "        bottom *= 4\n",
    "        left *= 4\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        # name=(\"{}{:.2f}\".format(name,accuracy))\n",
    "        name = \"{} {:.2f}\".format(name, accuracy)\n",
    "        # size = (right-left)/220\n",
    "        cv2.putText(frame, name, (left-10,top-6), font, 0.8, (255, 255, 255), 1)\n",
    "        #print(name)\n",
    "        # end_time=time.time()\n",
    "        # fps = 1/(end_time-start_frame)\n",
    "        # cv2.rectangle(frame,(15,20),(125,45),(0,255,255),-1)\n",
    "        # fps =\"fps:{:.2f}\".format(fps)\n",
    "\n",
    "        # cv2.putText(frame,fps,(16,40),font,0.7,(0,0,0),1)\n",
    "\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "webcam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a587c54-57c3-4f12-ad51-6027834eba66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vgg16",
   "language": "python",
   "name": "vgg16"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
